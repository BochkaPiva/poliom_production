# services/shared/utils/simple_rag.py

import logging
import numpy as np
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from sqlalchemy.orm import Session
from sqlalchemy import text

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
try:
    from services.shared.models.document import Document, DocumentChunk
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞—è, –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω
    from models.document import Document, DocumentChunk

from .llm_client import SimpleLLMClient, LLMResponse

logger = logging.getLogger(__name__)

class SimpleRAG:
    """
    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞
    - –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    - GigaChat –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    - –ù–∏–∫–∞–∫–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç–µ–π!
    """
    
    def __init__(self, db_session: Session, gigachat_api_key: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
        
        Args:
            db_session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            gigachat_api_key: API –∫–ª—é—á –¥–ª—è GigaChat
        """
        self.db_session = db_session
        self.llm_client = SimpleLLMClient(gigachat_api_key)
        self.similarity_threshold = 0.5  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä
        self.logger = logging.getLogger(__name__)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.embedding_model = SentenceTransformer('cointegrated/rubert-tiny2')
        self.logger.info("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            embedding = self.embedding_model.encode([text])[0]
            return embedding.tolist()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return []
    
    def search_relevant_chunks(self, question: str, limit: int = 15) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # 1. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            question_embedding = self.create_embedding(question)
            self.logger.info(f"–°–æ–∑–¥–∞–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(question_embedding)}")
            
            # 2. –í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –≤—ã—Å–æ–∫–∏–º –ø–æ—Ä–æ–≥–æ–º –∫–∞—á–µ—Å—Ç–≤–∞
            self.logger.info("–ü—ã—Ç–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pgvector –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            query = text("""
                SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                       1 - (dc.embedding <=> :embedding) as similarity,
                       dc.content_length
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                  AND dc.embedding IS NOT NULL
                  AND dc.content_length > 200
                  AND dc.content_length < 3000
                  AND dc.content NOT ILIKE '%–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ%'
                  AND dc.content NOT ILIKE '%—É—Ç–≤–µ—Ä–∂–¥–∞—é%'
                  AND dc.content NOT ILIKE '%–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä%'
                  AND dc.content NOT ILIKE '%—Å–∏—Å—Ç–µ–º–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞%'
                  AND dc.content NOT ILIKE '%–ø–æ–ª–æ–∂–µ–Ω–∏–µ%–æ%'
                  AND dc.content NOT ILIKE '%–≤–≤–µ–¥–µ–Ω–æ –≤–ø–µ—Ä–≤—ã–µ%'
                  AND dc.content NOT ILIKE '%–¥–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è%'
                ORDER BY dc.embedding <=> :embedding
                LIMIT :limit
            """)
            
            result = self.db_session.execute(query, {
                'embedding': str(question_embedding),
                'limit': limit * 2
            })
            
            vector_chunks = []
            for row in result:
                # –ü–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                if (row.similarity > 0.65 and 
                    self._is_relevant_content(row.content, question)):
                    vector_chunks.append({
                        'id': row.id,
                        'document_id': row.document_id,
                        'chunk_index': row.chunk_index,
                        'content': row.content,
                        'similarity': row.similarity,
                        'search_type': 'vector',
                        'content_length': row.content_length
                    })
            
            self.logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(vector_chunks)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # 3. –î–æ–ø–æ–ª–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–æ–∏—Å–∫–æ–º
            text_chunks = []
            
            if len(vector_chunks) < limit:
                keywords = self._extract_keywords(question)
                
                if keywords:
                    self.logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keywords}")
                    
                    # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    conditions = []
                    params = {}
                    
                    for i, keyword in enumerate(keywords[:5]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                        param_name = f'keyword_{i}'
                        conditions.append(f"dc.content ILIKE :{param_name}")
                        params[param_name] = f'%{keyword}%'
                    
                    text_query = text(f"""
                        SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                                0.6 as similarity, dc.content_length
                        FROM document_chunks dc
                        JOIN documents d ON dc.document_id = d.id
                        WHERE d.processing_status = 'completed'
                          AND dc.content_length > 200
                          AND dc.content_length < 3000
                          AND dc.content NOT ILIKE '%–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ%'
                          AND dc.content NOT ILIKE '%—É—Ç–≤–µ—Ä–∂–¥–∞—é%'
                          AND dc.content NOT ILIKE '%–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä%'
                          AND dc.content NOT ILIKE '%—Å–∏—Å—Ç–µ–º–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞%'
                          AND dc.content NOT ILIKE '%–ø–æ–ª–æ–∂–µ–Ω–∏–µ%–æ%'
                          AND dc.content NOT ILIKE '%–≤–≤–µ–¥–µ–Ω–æ –≤–ø–µ—Ä–≤—ã–µ%'
                          AND dc.content NOT ILIKE '%–¥–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è%'
                          AND ({' OR '.join(conditions)})
                        ORDER BY dc.content_length DESC
                        LIMIT :limit
                    """)
                    
                    params['limit'] = limit
                    text_result = self.db_session.execute(text_query, params)
                    
                    for row in text_result:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ—Ç —á–∞–Ω–∫ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        if (not any(chunk['id'] == row.id for chunk in vector_chunks) and
                            self._is_relevant_content(row.content, question)):
                            
                            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                            content_words = set(row.content.lower().split())
                            question_words = set(question.lower().split())
                            overlap = len(content_words & question_words)
                            
                            if overlap >= 2:  # –ú–∏–Ω–∏–º—É–º 2 –æ–±—â–∏—Ö —Å–ª–æ–≤–∞
                                text_chunks.append({
                                    'id': row.id,
                                    'document_id': row.document_id,
                                    'chunk_index': row.chunk_index,
                                    'content': row.content,
                                    'similarity': 0.6 + (overlap * 0.05),  # –ë–æ–Ω—É—Å –∑–∞ –±–æ–ª—å—à–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                                    'search_type': 'text',
                                    'content_length': row.content_length
                                })
                    
                    self.logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(text_chunks)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            all_chunks = vector_chunks + text_chunks
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
            all_chunks.sort(key=lambda x: x['similarity'], reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_chunks = all_chunks[:limit]
            
            if not final_chunks:
                self.logger.info("–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._fallback_search(question, limit)
            
            return final_chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ search_relevant_chunks: {str(e)}")
            return self._fallback_search(question, limit)
    
    def _extract_keywords(self, question: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        synonyms = {
            '–∞–≤–∞–Ω—Å': ['–∞–≤–∞–Ω—Å', '–∞–≤–∞–Ω—Å–æ–≤–∞—è', '–∞–≤–∞–Ω—Å–æ–≤—ã–π', '–ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å', '–ø–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞', '–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞'],
            '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ', '–∑–ø'],
            '–≤—ã–ø–ª–∞—Ç–∞': ['–≤—ã–ø–ª–∞—Ç–∞', '–≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è', '–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ', '–≤—ã–¥–∞—á–∞'],
            '–¥–∞—Ç–∞': ['–¥–∞—Ç–∞', '—á–∏—Å–ª–æ', '—Å—Ä–æ–∫', '–≤—Ä–µ–º—è', '–∫–æ–≥–¥–∞', '–¥–µ–Ω—å'],
            '—Ä–∞–∑–º–µ—Ä': ['—Ä–∞–∑–º–µ—Ä', '—Å—É–º–º–∞', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å–∫–æ–ª—å–∫–æ', '–≤–µ–ª–∏—á–∏–Ω–∞'],
            '–æ—Ç–ø—É—Å–∫': ['–æ—Ç–ø—É—Å–∫', '–æ—Ç–ø—É—Å–∫–Ω—ã–µ', '–æ—Ç–¥—ã—Ö', '–∫–∞–Ω–∏–∫—É–ª—ã'],
            '–±–æ–ª—å–Ω–∏—á–Ω—ã–π': ['–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '–±–æ–ª–µ–∑–Ω—å', '–Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '–ª–∏—Å—Ç –Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏'],
            '–ø—Ä–µ–º–∏—è': ['–ø—Ä–µ–º–∏—è', '–±–æ–Ω—É—Å', '–ø–æ–æ—â—Ä–µ–Ω–∏–µ', '–Ω–∞–¥–±–∞–≤–∫–∞'],
            '–¥–æ–≥–æ–≤–æ—Ä': ['–¥–æ–≥–æ–≤–æ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä'],
            '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': ['—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ', '—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ', '–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ', '—É—Ö–æ–¥'],
            '–≥—Ä–∞—Ñ–∏–∫': ['–≥—Ä–∞—Ñ–∏–∫', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '—Ä–µ–∂–∏–º', '–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'],
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã': ['–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Å–ø—Ä–∞–≤–∫–∏', '–±—É–º–∞–≥–∏', '—Ñ–æ—Ä–º—ã']
        }
        
        question_lower = question.lower()
        keywords = set()
        
        # –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        for base_word, word_list in synonyms.items():
            for word in word_list:
                if word in question_lower:
                    keywords.update([base_word])  # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ —Å–ª–æ–≤–æ
                    keywords.add(word)  # –ò —Å–∞–º–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
                    break
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–∞ (–¥–∞—Ç—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã)
        import re
        numbers = re.findall(r'\b\d{1,2}\b', question)
        for num in numbers:
            keywords.add(num)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', question_lower)
        stop_words = {'–∫–æ–≥–¥–∞', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—Å–∫–æ–ª—å–∫–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–æ—Ç–∫—É–¥–∞', '–∫—É–¥–∞'}
        for word in words:
            if word not in stop_words:
                keywords.add(word)
        
        return list(keywords)[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def _fallback_search(self, question: str, limit: int) -> List[Dict]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            self.logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback")
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            query = text("""
                SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                       0.5 as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                  AND (dc.content ILIKE :search_term1 
                       OR dc.content ILIKE :search_term2
                       OR dc.content ILIKE :search_term3)
                LIMIT :limit
            """)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            words = question.lower().split()
            search_terms = [f'%{word}%' for word in words if len(word) > 2][:3]
            
            if not search_terms:
                return []
            
            params = {
                'search_term1': search_terms[0] if len(search_terms) > 0 else '%',
                'search_term2': search_terms[1] if len(search_terms) > 1 else search_terms[0],
                'search_term3': search_terms[2] if len(search_terms) > 2 else search_terms[0],
                'limit': limit
            }
            
            result = self.db_session.execute(query, params)
            
            chunks = []
            for row in result:
                chunks.append({
                    'id': row.id,
                    'document_id': row.document_id,
                    'chunk_index': row.chunk_index,
                    'content': row.content,
                    'similarity': row.similarity,
                    'search_type': 'fallback'
                })
            
            self.logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ fallback –ø–æ–∏—Å–∫–µ: {str(e)}")
            return []
    
    def format_context(self, chunks: List[DocumentChunk]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = self.db_session.query(Document).filter(
                Document.id == chunk['document_id']
            ).first()
            
            doc_title = document.title if document else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"
            
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {doc_title}]\n{chunk['content']}\n"
            )
        
        return "\n".join(context_parts)
    
    def answer_question(self, 
                       question: str,
                       user_id: Optional[int] = None) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            Dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å: {question[:100]}...")
            
            # 1. –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            relevant_chunks = self.search_relevant_chunks(question, limit=20)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
            
            if not relevant_chunks:
                return {
                    'answer': '–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ HR-–æ—Ç–¥–µ–ª—É.',
                    'sources': [],
                    'chunks': [],
                    'files': [],
                    'success': True,
                    'tokens_used': 0
                }
            
            # 2. –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –±–µ—Ä–µ–º –ª—É—á—à–∏–µ —á–∞–Ω–∫–∏
            top_chunks = relevant_chunks[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –ª—É—á—à–∏—Ö —á–∞–Ω–∫–æ–≤
            context = self.format_context(top_chunks)
            
            # –û–¢–õ–ê–î–ö–ê: –í—ã–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ª–æ–≥
            self.logger.info(f"üîç –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø LLM (–¥–ª–∏–Ω–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤):")
            self.logger.info("="*80)
            self.logger.info(context[:2000] + "..." if len(context) > 2000 else context)
            self.logger.info("="*80)
            
            # 3. –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
            enhanced_prompt = f"""
–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–º
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏ –≥–¥–µ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ
4. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, —Å—É–º–º—ã - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –∏—Ö
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
"""
            
            llm_response = self.llm_client.generate_answer(
                context=enhanced_prompt,
                question=question
            )
            
            if not llm_response.success:
                return {
                    'answer': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.',
                    'sources': [],
                    'chunks': [],
                    'files': [],
                    'success': False,
                    'error': llm_response.error,
                    'tokens_used': 0
                }
            
            # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Ñ–∞–π–ª—ã —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
            sources = []
            files = []
            seen_documents = set()
            
            for chunk in top_chunks:
                document = self.db_session.query(Document).filter(
                    Document.id == chunk['document_id']
                ).first()
                
                if document and document.title not in seen_documents:
                    sources.append({
                        'title': document.title,
                        'chunk_index': chunk['chunk_index'],
                        'document_id': document.id
                    })
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –¥–ª—è –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–∏—è
                    files.append({
                        'title': document.title,
                        'file_path': document.file_path,  # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                        'document_id': document.id,
                        'similarity': chunk['similarity'],
                        'file_size': document.file_size,
                        'file_type': document.file_type,
                        'original_filename': document.original_filename
                    })
                    
                    seen_documents.add(document.title)
            
            # 5. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            formatted_answer = self._post_process_answer(llm_response.text)

            # 6. –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if user_id:
                self._log_query(user_id, question, formatted_answer, len(relevant_chunks))
            
            return {
                'answer': formatted_answer,
                'sources': sources,
                'chunks': relevant_chunks,
                'files': files[:5],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ñ–∞–π–ª–æ–≤
                'success': True,
                'tokens_used': llm_response.tokens_used,
                'chunks_found': len(relevant_chunks),
                'context_length': len(context)
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ answer_question: {str(e)}")
            return {
                'answer': '–ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.',
                'sources': [],
                'chunks': [],
                'files': [],
                'success': False,
                'error': str(e),
                'tokens_used': 0
            }
    
    def _post_process_answer(self, answer: str) -> str:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            answer: –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
            
        Returns:
            str: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        answer = answer.strip()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ - —É–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–≤–æ–π–Ω—ã–µ
        lines = answer.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                cleaned_lines.append(line)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ–¥–Ω–∏–º –ø–µ—Ä–µ–Ω–æ—Å–æ–º –º–µ–∂–¥—É –Ω–∏–º–∏
        result = '\n'.join(cleaned_lines)
        
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        sentences = result.split('. ')
        unique_sentences = []
        seen_sentences = set()
        
        for sentence in sentences:
            sentence_clean = sentence.strip().lower()
            if sentence_clean and sentence_clean not in seen_sentences and len(sentence_clean) > 10:
                unique_sentences.append(sentence.strip())
                seen_sentences.add(sentence_clean)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        final_sentences = []
        for i, sentence in enumerate(unique_sentences):
            if not sentence.endswith('.') and not sentence.endswith(':') and not sentence.endswith(';'):
                if i < len(unique_sentences) - 1:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                    sentence += '.'
            final_sentences.append(sentence)
        
        return '. '.join(final_sentences)
    
    def _log_query(self, user_id: int, question: str, answer: str, chunks_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            from shared.models.query_log import QueryLog
            
            log_entry = QueryLog(
                user_id=user_id,
                query_text=question,
                response_text=answer,
                chunks_used=chunks_count,
                model_used="GigaChat"
            )
            
            self.db_session.add(log_entry)
            self.db_session.commit()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
    
    def health_check(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        return {
            'embeddings_model': self.embedding_model is not None,
            'llm_client': self.llm_client.health_check(),
            'database': self._check_database()
        }
    
    def _check_database(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            self.db_session.execute(text("SELECT 1"))
            return True
        except Exception:
            return False

    def _is_relevant_content(self, content: str, question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫ –≤–æ–ø—Ä–æ—Å—É"""
        content_lower = content.lower()
        question_lower = question.lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        technical_markers = [
            '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—É—Ç–≤–µ—Ä–∂–¥–∞—é', '–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä', 
            '—Å–∏—Å—Ç–µ–º–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞', '–≤–≤–µ–¥–µ–Ω–æ –≤–ø–µ—Ä–≤—ã–µ', '–¥–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è',
            '–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '–Ω–∞—Å—Ç–æ—è—â–µ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ',
            '–∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ', '—Å–∏–±–≥–∞–∑–ø–æ–ª–∏–º–µ—Ä'
        ]
        
        # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤, –∏—Å–∫–ª—é—á–∞–µ–º
        technical_count = sum(1 for marker in technical_markers if marker in content_lower)
        if technical_count > 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
        question_words = set(word for word in question_lower.split() if len(word) > 2)
        content_words = set(content_lower.split())
        
        overlap = question_words & content_words
        
        # –ú–∏–Ω–∏–º—É–º 2 –æ–±—â–∏—Ö —Å–ª–æ–≤–∞ –∏–ª–∏ –æ–¥–∏–Ω —Ç–æ—á–Ω—ã–π –∫–ª—é—á–µ–≤–æ–π —Ç–µ—Ä–º–∏–Ω
        if len(overlap) >= 2:
            return True
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        key_terms = {
            '–æ—Ç–ø—É—Å–∫': ['–æ—Ç–ø—É—Å–∫', '–æ—Ç–ø—É—Å–∫–Ω—ã–µ', '–æ—Ç–¥—ã—Ö'],
            '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞'],
            '–≤—ã–ø–ª–∞—Ç—ã': ['–≤—ã–ø–ª–∞—Ç—ã', '–≤—ã–ø–ª–∞—Ç–∞', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏—è', '–ø—Ä–µ–º–∏—è'],
            '—é–±–∏–ª–µ–π': ['—é–±–∏–ª–µ–π', '—é–±–∏–ª–µ–π–Ω—ã–µ', '–≥–æ–¥–æ–≤—â–∏–Ω–∞'],
            '–±–æ–ª—å–Ω–∏—á–Ω—ã–π': ['–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '–Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å'],
            '–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞': ['–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞', '—Å–ª—É–∂–µ–±–Ω–∞—è –ø–æ–µ–∑–¥–∫–∞'],
            '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': ['—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ', '—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞']
        }
        
        for term_group in key_terms.values():
            question_has_term = any(term in question_lower for term in term_group)
            content_has_term = any(term in content_lower for term in term_group)
            
            if question_has_term and content_has_term:
                return True
        
            return False 